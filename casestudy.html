<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Chronos: Case Study</title>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link rel="stylesheet" href="css/casestudy.css">
    <link rel="shortcut icon" type="image/png" href="/favicon.png" />
    <script src="https://cdn.jsdelivr.net/npm/waypoints@4.0.1/lib/noframework.waypoints.min.js"></script>
    <script src="javascripts/csWaypoints.js"></script>
  </head>
  <body>
    <nav>
      <a href="index.html" class="hvr-bob"></a>
      <ul>
        <li><a href="casestudy.html" class="hvr-float-shadow">Case Study</a></li>
        <li><a href="bibliography.html" class="hvr-float-shadow">Bibliography</a></li>
        <li><a href="team.html" class="hvr-float-shadow">Team</a></li>
        <li><a href="https://github.com/chronos-project" class="hvr-float-shadow"><img src="images/icons/chronos_github_gray.png" alt="Github logo" /></a></li>
      </ul>
    </nav>
    <main>
      <nav id="toc">
        <ul>
          <li class="h2 active" id="introduction-toc" data-element="introduction">Introduction</li>
          <li class="h2" id="what-is-event-data-toc" data-element="what-is-event-data-">What is Event Data?
            <ul>
              <li class="h3" data-element="event-data-vs-entity-data">Event Data vs Entity Data</li>
              <li class="h3" data-element="streams-and-tables">Streams and Tables</li>
            </ul>
          </li>
          <li class="h2" id="existing-solutions-toc" data-element="existing-solutions">Existing Solutions
            <ul>
              <li class="h3" data-element="yandex-metrica">Yandex Metrica</li>
              <li class="h3" data-element="eventhub">EventHub</li>
              <li class="h3" data-element="countly-community-edition">Countly Community Edition</li>
              <li class="h3" data-element="chronos">Chronos</li>
            </ul>
          </li>
          <li class="h2" id="capturing-events-toc" data-element="capturing-events">Capturing Events
            <ul>
              <li class="h3" data-element="payload-size">Payload Size</li>
              <li class="h3" data-element="beacon-api-and-error-handling">Beacon API and Error Handling</li>
              <li class="h3" data-element="security-concerns">Security Concerns</li>
            </ul>
          </li>
        </ul>
      </nav>
      <article>
        <div id='markdown'>
          <h1 id="case-study">Case Study</h1>
          <h2 id="introduction">Introduction</h2>
          <p>Chronos is an event-capturing framework for greenfield applications, and is built using NodeJS, Apache Kafka, TimescaleDB, and PipelineDB. It allows developers to easily capture and store user events that happen on the client side, and then perform data exploration on the captured events in order to aid business logic. By using Apache Kafka, Chronos is a streaming system at its core and is thus easily expandable to the developer's needs. Further, Chronos is deployed using Docker and comes with a CLI that abstracts the difficulties in installing and running the system.</p>
          <p>When building Chronos we faced several difficulties. The first were the various challenges in sending event data to the API server including limited browser resources, security concerns, and payload size limitations. Second was the issue of code coupling, or making sure that Chronos could easily be expanded to fit a developers needs. Third, we had to overcome the difficulties with abstracting away any difficulties for the developer when working with Apache Kafka, including installation, configuration, and potential errors. Lastly, we had to overcome the difficulties in storing event data while still making data exploration possible for a large range of developers.</p>
          <p>This case study will begin by describing what event data is and how it contrasts from entity data. Next, we will define what a greenfield application is and review some of the existing solutions for this area and what some problems there are in using these systems. Lastly, we will describe how we went about building Chronos in order to solve these problems and how we overcame the challenges that presented themselves along the way.</p>
          <h2 id="what-is-event-data-">What is Event Data?</h2>
          <h3 id="event-data-vs-entity-data">Event Data vs Entity Data</h3>
          <p>Very often, when we speak of "data" in an application we generally tend to think of data that resides in a database that is modeled after real world entities. This may be a shopping cart, a bank account, a character in an online video game, etc. In each of these cases, we are dealing with **entity data**, or data that describes the current state of some entity. This type of data is generally what comes to mind when we think of SQL and Relational Databases.</p>
          <table>
            <thead>
              <tr>
                <th>Column Name</th>
                <th>Value</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>id</td>
                <td>327</td>
              </tr>
              <tr>
                <td>account_name</td>
                <td>Bugman27</td>
              </tr>
              <tr>
                <td>email</td>
                <td>imabug@foo.bar</td>
              </tr>
              <tr>
                <td>account_status</td>
                <td>active</td>
              </tr>
              <tr>
                <td>name</td>
                <td>Franz Kafka</td>
              </tr>
              <tr>
                <td>age</td>
                <td>53</td>
              </tr>
            </tbody>
          </table>
          <p class="caption">Typical example of entity data</p>
          <p>However, there is an emerging realization that while we genreally tend to think of applications as tracking entity data, there is also a constantly occuring stream of events. An <strong>event</strong> in this case is any action that occurs in an application. Examples could be a user clicking on a link, submitting a payment, creating a character, landing on a page, etc.</p>
          <div class="code-block">
            <pre>
              <code class="language-json">
{
  <span class="json-key">"eventType"</span>: <span class="json-value-string">"pageview"</span>,
  <span class="json-key">"timestamp"</span>: <span class="json-value-string">"2018 20:29:48 GMT-0600"</span>,
  <span class="json-key">"page URL"</span>: <span class="json-value-string">"www.example.com"</span>,
  <span class="json-key">"pageTitle"</span>: <span class="json-value-string">"Example Title"</span>,
  <span class="json-key">"user"</span>: {
    <span class="json-key">"userId"</span>: <span class="json-value-number">7689476946</span>,
     <span class="json-key">"userCountry"</span>: <span class="json-value-string">"USA"</span>,
     <span class="json-key">"userLanguage"</span>: <span class="json-value-string">"en-us"</span>,
     <span class="json-key">"userAgent"</span>: <span class="json-value-string">"Chrome"</span>,
     ...
}
              </code>
            </pre>
          </div>
          <p class="caption">Example of a <code>pageview</code> event as a JSON object</p>
          <p>As such, <strong>event data</strong> is data that models each of these events. Unlike entity data which is core to the business logic of an application, event data is a kind of metadata, or data about data, which describes how an application is being used, and is usually not central to its business logic. In this case, if entity data is a noun which carries around state, then event data is a verb which describes an action.</p>
          <p>Since event data describes how an application is being used, capturing and analyzing this data provides a major competitive advantage since it can be used to positively iterate on a product and increase profits.[1] However, if a developer wants to track event data, then unlike entity data, all the events would have to be stored since it is with the total set of events that one can analyze the data and draw conclusions. In other words, events are treated as immutable and are never updated and rarely deleted. The following table maps out the differences in this model between event and entity data thus far:[2]</p>
          <table>
            <thead>
              <tr>
                <th>Entity Data</th>
                <th>Event Data</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Strict schema</td>
                <td>Felxible schema</td>
              </tr>
              <tr>
                <td>Normalized</td>
                <td>Denormalized</td>
              </tr>
              <tr>
                <td>Shorter</td>
                <td>Wider</td>
              </tr>
              <tr>
                <td>Describes nouns</td>
                <td>Describes verbs</td>
              </tr>
              <tr>
                <td>Describes now</td>
                <td>Describes trends over time</td>
              </tr>
              <tr>
                <td>Updates</td>
                <td>Appends</td>
              </tr>
              <tr>
                <td>O(N)</td>
                <td>O(N * K)</td>
              </tr>
            </tbody>
          </table>
          <p>While this is a good first attempt to model events, the key problem with ending here is that while there is definitely a legitimate distinction between event data and entity data, the two cannot be completely partitioned into unrelated categories. To understand why this is, we need to discuss the &quot;theory of streams and tables&quot; as concevied of by figures such as Martin Kleppmann, Jay Kreps, and Tyler Akidau.</p>
          <h3 id="streams-and-tables">Streams and Tables</h3>
          <p>To start with, we&#39;ll need some working definitions of what streams and tables are. To borrow from Akidau, a <strong>stream</strong> is &quot;[a]n element-by-element view of the evolution of a dataset over time.&quot;[3] Streams have traditionally been processed by streaming systems that are designed to handle unbounded (infinite) datasets. A <strong>table</strong> is &quot;[a] holistic view of a dataset at a specific point in time&quot;[2] and is traditionally handled within relational databases (i.e. SQL). We can expand these definitions by saying that streams are data that are in motion, while tables are data that are at rest.</p>
          <p>Staying with tables for a moment, it&#39;s worth remembering that the underlying data structure for many databases is a log, more specifically one that is append-only. As each transaction takes place for a particular entity, they are recorded to the log. The log, then, can be seen as a kind of stream of data which can be used to re-create the state of a table within our database. More broadly speaking, the aggregation of a stream will give us a table.</p>
          <p>(diagram)</p>
          <p>The inverse of this relationship is that streams are created from tables as a kind of change-log for the table. In other words, if we look at the changes that occur on a particular entity over time, we end up with a stream of data.</p>
          <p>(diagram)</p>
          <p>To bring this back to event data and entity data: if we were to write all the event data within our application onto a log, then we would be able to re-create the state of any entity data that we needed. In other words, rather than being two separate kinds of data, event data are the individual data points that make up the stream of our application, while entity data are the aggregated snapshots of our stream of event data.</p>
          <p>In this respect, though databases are often thought of as the &quot;source of truth&quot; in an application, they actually contain just a set of aggregations of event data at a particular point in time. It is the log that holds the event data that is the actual source of truth of our application since it contains the fundamental building blocks to recreate the state of the application.</p>
          <p>(image)</p>
          <p>Given all this, it should be clear that there are several good reasons for capturing event data:</p>
          <ol>
            <li>Event data provide rich information that can be used to see how users are using your application</li>
            <li>Event data can validate business logic or be used to form new strategies</li>
            <li>Event data are the fundamental building blocks of the state of an application</li>
          </ol>
          <p>The main difficulty in capturing and using event data is that since for so long they&#39;ve been seen to exist implicitly in an application they don&#39;t have a proper place within the architecture of many set ups. Further, since events are constantly happening in real time, any system for capturing and storing event data needs to be able to map to this stream of events and store data in an apropos way.</p>
          <p>Thankfully, streaming architectures for web applications have been developing at a quick pace over the last decade and a half since the release of MapReduce in 2003. However, they are a specialized field and have a significant learning curve to use, let alone to then go and implement them for production. It would be unrealistic for ever developer to learn how to implement such a system for capturing event data, especially for greenfield applications when the developer isn&#39;t even sure how the application will be used yet.</p>
          <h2 id="existing-solutions">Existing Solutions</h2>
          <p>Luckily, there are already existing solutions for a developer who wishes to capture, store, and analyze event data. However, many of these solutions are proprietary in nature and while they could be used for greenfield applications, they are better suited for larger or enterprise level applications. With these solutions we generally found the following problems:</p>
          <ol>
            <li>Monetary costs</li>
            <li>Data lives on the proprietary service&#39;s servers</li>
            <li>Data may be sampled</li>
            <li>You may not have access to your raw data</li>
            <li>Manual implementation of events to capture</li>
          </ol>
          <p>The justifications of these drawbacks should be straightforward:</p>
          <ol>
            <li>Since greenfield applications are usually in a prototype or new phase, they likely don&#39;t have or want to spend a lot of money on proprietary solutions</li>
            <li>With the growing concern about how people&#39;s data is being used, it&#39;s always a gamble to have your data hosted on a service&#39;s server that you don&#39;t have direct access to</li>
            <li>Same problem as #2</li>
            <li>If you can only access data through an API and can never get at the raw data itself, not only does that limit what you can do with the data, but it makes it hard if not impossible to transfer it to another solution</li>
            <li>Since a greenfield application doesn&#39;t yet know what events to capture, requiring manual implementation of event capturing is counter-intuitive</li>
          </ol>
          <p>Of the various solutions, there were three that we found in particular better suited our use case: Yandex Metrica, Event Hub, and Countly Community Edition.</p>
          <h3 id="yandex-metrica">Yandex Metrica</h3>
          <p>One existing solution is provided by <a href="https://metrica.yandex.com/about?">Yandex Metrica</a>, which is a product of the larger <a href="https://yandex.com/">Yandex</a> company. Two of the standout advantages of Yandex Metrica is that they provide the ability to create &quot;click maps&quot;, visual representations on a web page where a user clicked through, as well as heat maps that show which sections of your pages have the most activity. Yandex also has no fee associated with it.</p>
          <p>While Yandex does require your data to live on their servers, they are required to respect European data privacy laws and also encrypt your data so it can&#39;t be used by other analytical services. Further, any data they look at for their own analytical purposes is done so in a way that your data remains anonymous.</p>
          <p>One big drawback is that Yandex requires most event capturing to be manually implemented. Another problem is that while you do have access to your raw data, you can only access via their API, and so you have to extract and store your data manually.</p>
          <h3 id="eventhub">EventHub</h3>
          <p>Another solution for Greenfield applications is <a href="https://github.com/Codecademy/EventHub">EventHub</a>, which is an open source event tracking/storage system written in Java. It has some impressive analytical capabilities such as cohort and funneling queries, but it has little-to-no automatic tracking of events.</p>
          <p>Two other drawbacks of EventHub are that it&#39;s timestamp is processing time only (i.e. it logs the timestamp when the data hits the server as opposed to when it occurs in the client). This is mitigated by the fact that any event capturing a developer implements can just include a client-side timestamp. However, the largest issue is that the project has been abandoned for 5 years now, so support would likely be totally absent.</p>
          <h3 id="countly-community-edition">Countly Community Edition</h3>
          <p>Of the three choices, <a href="https://github.com/Countly/countly-server">Countly&#39;s community edition</a> (open source) was the strongest option. Countly allows not only for a quick manual setup on your own server, but also provides a one-click setup option for hosting your server on Digital Ocean. Their tracker is a JavaScrpit SDK that tracks the following events automatically:</p>
          <ul>
            <li>sessions</li>
            <li>pageviews (both for tradition websites and single page applications)</li>
            <li>link clicks (both on a link or those on a parent node)</li>
            <li>form submissions</li>
          </ul>
          <p>Two other events, mouse clicks and mouse scrolls, are only automatically captured in the enterprise edition. Since Countly must be ran on a server you own, you have access to all of your own data (which is stored in MongoDB), and they also provide a UI for visualizing and exploring your data.</p>
          <p>The largest drawbacks to Countly are the limited number of events captured (anything else must be implemented manually) and that their pipeline setup is a direct connection from their API to their database (this is also true of EventHub). Why this latter design is problematic will be discussed below.</p>
          <h3 id="chronos">Chronos</h3>
          <p>For Chronos to be an alternative in this space, it must solve the problems listed above as well as provide some benefits compared to the existing solutions. In the end, Chronos was able solve the 5 problems above:</p>
          <ol>
            <li>Chronos is open source, and thus free to use</li>
            <li>Data only exists on the server you host Chronos on and so you don&#39;t have to fear its security</li>
            <li>Chronos will never sample your data since it&#39;s just an infrastructure</li>
            <li>Chronos provides access to your raw data</li>
            <li>Chronos provides a config file that specifies which events you&#39;d like to capture: everything else is automated</li>
          </ol>
          <p>In addition to this, we provided a way for Chronos to visualize any queries over the data. We also wanted to make sure that Chronos would be space efficient since a greenfield application shouldn&#39;t be spending lots of money on their own server to collect data. Below we detail how we went about building Chronos and the challenges we faced.</p>
          <h2 id="capturing-events">Capturing Events</h2>
          <p>Chronos captures events on the client side by using a <code>tracker.js</code> file. The tracker itself is compiled from multiple files by using Browserify and automates the capturing of the following events:</p>
          <ul>
            <li>pageviews</li>
            <li>mouse movements</li>
            <li>mouse clicks</li>
            <li>link clicks</li>
            <li>key presses</li>
            <li>form submissions</li>
          </ul>
          <p>In addition to this, the tracker captures metadata about the page in which the events occur, such as:</p>
          <ul>
            <li>the url of the page</li>
            <li>the title of the page</li>
            <li>the user agent</li>
            <li>the language of the browser</li>
            <li>whether cookies are allowed</li>
            <li>a uuid</li>
          </ul>
          <p>The tracker first checks to see if a uuid exists within the <code>window.sessionStorage</code> object. If not, it creates one and stores it there.</p>
          <p>While capturing most of these events did not prove difficult, we did encounter difficulties when capturing mouse movements. Our first attempt was to add an event listener to the <code>mousemove</code> event which would store the <code>x</code> and <code>y</code> coordinates of the mouse in an object. We then had a <code>setInterval()</code> call that checked every 10ms whether the mouse had moved from its previous position, and to both record it if it did and reset the current position to this new location</p>
          <div class="code-block">
            <pre>
              <code class="language-javascript">
<span class="hl-purple">let</span> mousePos;
<span class="hl-purple">let</span> prevMousePos;

<span class="hl-red">document</span>.<span class="hl-teal">addEventListener</span>(<span class="hl-green">&#39;mousemove&#39;</span>, (<span class="hl-red">event</span>) <span class="hl-purple">=&gt;</span> {
  mousePos <span class="hl-teal">=</span> {
    x: <span class="hl-red">event</span>.<span class="hl-red">clientX</span>,
    y: <span class="hl-red">event</span>.<span class="hl-red">clientY</span>,
  }
});

<span class="hl-teal">setInterval</span>(() <span class="hl-purple">=&gt;</span> {
  <span class="hl-purple">const</span> <span class="hl-orange">pos</span> <span class="hl-teal">=</span> mousePos;

  <span class="hl-purple">if</span> (pos) {
    <span class="hl-purple">if</span> (<span class="hl-teal">!</span>prevMousePos <span class="hl-teal">||</span> prevMousePos <span class="hl-teal">&amp;&amp;</span> pos <span class="hl-teal">!==</span> prevMousePos) {

      prevMousePos <span class="hl-teal">=</span> pos;
    }
  }
}, <span class="hl-orange">100</span>);
              </code>
            </pre>
          </div>
          <p>By using <code>setInterval()</code> we were able to standardize the amount of data we were capturing in the <code>mousemovement</code> event across browsers since each has their own implementation of the rate of capture.</p>
          <p>However, when we began to try and send the events directly to the API, we began raising <code>net::ERR_INSUFFICIENT_RESOURCES</code> exceptions in certain browsers. We first attempted to reduce the granularity of the event to 20ms and then to 1000ms but kept running into the same error. Furthermore, on browsers that did not have this problem, the API began to choke on the sheer number of requests very quickly.</p>
          <p>(graphic goes here)</p>
          <p>A solution to this is to send the events in batch by storing them in a buffer and then sending the buffer when either a) it is full, or b) when the user begins to leave the page. Once we did this both the browser and the API stopped experiencing issues.</p>
          <p>(graphic goes here)</p>
          <h3 id="payload-size">Payload Size</h3>
          <p>(server specs, etc)</p>
          <p>In our the first design of our buffer, we sent each event over to the server as a JSON object. We tested the size of the events by sending a buffer with only link click events with the same values along with a consistent write key and metadata object. In this scenario, each event was roughly 92 bytes in size, and so a buffer containing 1,150 events would return a <code>413</code> error from the server.</p>
          <p>(show graphic of buffer with JSON)</p>
          <p>While a max buffer size of 1,150 events seems reasonable, we wanted to make sure to get as large of a payload as possible in order to ping the API less often. 1,150 events isn&#39;t as many as it may first seem when you remember that Chronos captures certain events such as key presses and mouse movements at a very small granularity. This problem will only increase in future iterations as Chronos captures even more events.</p>
          <p>We were able to optimize the buffer by removing the keys of the JSON object and instead sending all of the values in a nested array. Since the value at index <code>0</code> is the name of the event, we could use that information on the server side to write the data to the appropriate table in the databases. By doing this, we reduced the size of each of the events to roughly 42 bytes, allowing us to increase the maximum buffer size to over 2,500 events (a 100%+ increase in payload).</p>
          <p>(show graphic of buffer with arrays)</p>
          <p>While our next thought was to serialize the data into binary, a key problem is that we could never guarantee the size of our buffer, nor the values of the metadata object, nor the write key. As such, optimizing on binary would be unfeasible. We could instead just serialize the entire UTF-16 string into binary, but the minimum byte size for each character would be 8 bits which ended up being no more effective than just sending the string as is.</p>
          <h3 id="beacon-api-and-error-handling">Beacon API and Error Handling</h3>
          <p>By default, the tracker sends the data to the API server by using the Beacon API. This API was designed with analytics and diagnostics in mind as it allows for data to be sent in an asynchronous <code>POST</code> request that is non-blocking and thus doesn&#39;t interfere with the user&#39;s experience of the application. However, the Beacon API doesn&#39;t support error handling since it doesn&#39;t usually require to receive a response from the server. To handle errors, Chronos also allows for the Fetch API to be used.</p>
          <h3 id="security-concerns">Security Concerns</h3>
          <p>Since the tracker file lives on the client side, it presents inherent difficulties with security since the code can always be examined. As such, a malicious user can exploit the tracker to send corrupt data. However, we found from our research that this is an inherent difficulty within the field of web analytics:</p>
          <blockquote>
            <p>Thereâ€™s no way to both allow clients to send data to Keen and prevent a malicious user of a client from sending bad data. This is an unfortunate reality. Google Analytics, Flurry, KissMetrics, etc., all have this problem. <em><a href="https://keen.io/docs/security/">Keen IO</a></em></p>
          </blockquote>
          <p>As such, we provided two layers of security. The first is that we provide a write key which exists both on the server side and is imbedded withint the tracker when it is compiled. When data is sent to the server we use middleware to check if the write key in the client matches that of the server. If it doesn&#39;t, the request is rejected. This way, if a developer notices that bad data is coming through to the server, the api key can be re-generated and thus prevent the malicious writes from coming through.</p>
          <p>The second layer of security is that another piece of middleware contains a listing of permitted host addresses that can write data to the server. If the incoming request comes from a host that isn&#39;t white listed, the server rejects the request. </p>

        </div>
      </article>
    </main>
    <footer>
      <p>Current Version: 0.9.0</p>
      <p><a href="casestudy.html" class="hvr-float-shadow">Case Study</a> | <a href="bibliography.html" class="hvr-float-shadow">Bibliography</a> | <a href="team.html" class="hvr-float-shadow">Team</a></p>
    </footer>
  </body>
</html>
